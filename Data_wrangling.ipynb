{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/pUs1dYLtlsqcaahbzWoC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pallavrouth/MarketingAnalytics/blob/main/Data_wrangling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is data wrangling?\n",
        "\n",
        "Data wrangling, also known as data munging or data preprocessing, refers to the process of cleaning, transforming, and organizing raw and messy data into a structured and usable format for analysis and modeling.\n",
        "\n",
        "It is a crucial step in the data analysis pipeline as it ensures that the data is accurate, consistent, and properly formatted before any meaningful insights can be extracted from it.\n",
        "\n",
        "# What does it include?\n",
        "\n",
        "**Data Collection:** Gather raw data from various sources, such as databases, files, APIs, or web scraping.\n",
        "\n",
        "**Data Inspection**: Explore the data to understand its structure, format, and potential issues. Identify missing values, outliers, and inconsistencies.\n",
        "\n",
        "**Data Cleaning:** Address data quality issues by handling missing values, correcting errors, and dealing with outliers. This may involve imputing missing values and removing duplicates\n",
        "\n",
        "**Data Transformation:** This may include changing the shape of the data or converting the data types, creating new variables, aggregating data, and creating new features.\n",
        "\n",
        "**Data Enrichment:** Enhance the dataset by adding relevant information from external sources or combining multiple datasets to gain more insights.\n",
        "\n",
        "Tools and libraries such as Python's Pandas, NumPy, and scikit-learn are commonly used for data wrangling tasks, as they provide powerful functionalities for data manipulation, cleaning, and transformation."
      ],
      "metadata": {
        "id": "gRWiCmTI4Q8_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pandas\n",
        "\n",
        "Pandas is a popular open-source Python library used for data manipulation and analysis. It provides data structures and functions that make it easier to work with structured, tabular, and time-series data."
      ],
      "metadata": {
        "id": "jls83EC74WrV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "EY9_P2Eo6z48"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data structures\n",
        "\n",
        "Pandas **Series** and **DataFrames** are two fundamental data structures provided by the Pandas library for handling and manipulating data in Python. They are designed to work with tabular and labeled data, making it easier to perform various data analysis and manipulation tasks."
      ],
      "metadata": {
        "id": "RSHMQZ9a6VGr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1P6eXFf4GWR"
      },
      "outputs": [],
      "source": [
        "# series - is a one-dimensional labeled array capable of holding any data type\n",
        "# (integers, strings, floating point numbers, Python objects\n",
        "customer_ages_array = pd.Series([30, 25, 28, 22, 35], index = ['Alice', 'Bob', 'Charlie', 'David', 'Eve'])\n",
        "customer_ages_dict = pd.Series({'Alice':30, 'Bob': 25, 'Charlie': 28, 'David': 22, 'Eve': 35})\n",
        "print(customer_ages_array)\n",
        "print(customer_ages_dict)\n",
        "print(type(customer_ages_dict))\n",
        "\n",
        "# dataframe - 2-dimensional labeled data structure with columns of potentially different types.\n",
        "# You can think of it like a spreadsheet or SQL table, or a dict of Series objects.\n",
        "data = {\n",
        "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
        "    'Age': [30, 25, 28, 22, 35],\n",
        "    'Country': ['USA', 'Canada', 'UK', 'Australia', 'USA'],\n",
        "    'PurchaseAmount': [100, 50, 80, 120, 90]\n",
        "}\n",
        "\n",
        "customer_data = pd.DataFrame(data)\n",
        "print(customer_data)\n",
        "print(type(customer_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this course we will mainly deal with dataframes. Also we will mostly import datasets instead of creating them.\n",
        "\n",
        "# Importing data\n",
        "\n",
        "Pandas provides import functions for variety of file types. In this course we will mainly deal with csv files and sometimes with excel files. For csv we use `read_csv()` and for excel we use `read_excel()` function.\n",
        "\n"
      ],
      "metadata": {
        "id": "ppNi6UOmCF_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# file path\n",
        "file_path = \"https://raw.githubusercontent.com/pallavrouth/MarketingAnalytics/main/datasets/transaction_transactions.csv\"\n",
        "\n",
        "# read the CSV file into a DataFrame\n",
        "transactions = pd.read_csv(file_path, index_col = False)\n",
        "print(transactions.head())\n",
        "\n",
        "# to specify a particular column as index\n",
        "transactions_withid = pd.read_csv(file_path, index_col = 'customer_id')\n",
        "print(transactions_withid.head())"
      ],
      "metadata": {
        "id": "8qhYsuaHIc3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes, when importing pandas creates an index column. We can avoid this by setting `index_col = 0`\n",
        "\n",
        "# Inspecting the data\n",
        "\n",
        "After importing the dataset, the first task is to inspect the data. Inspection of the data involves\n",
        "\n",
        "1. understanding the dimensions of the data,\n",
        "2. understanding the data types for each column\n",
        "3. understanding whether there are problems/issues with the data (for example, presence of missing values)"
      ],
      "metadata": {
        "id": "p6bgoEKGLuW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get a sneak peak into the data\n",
        "print(transactions.head(), '\\n') # alternatively use tail()\n",
        "\n",
        "# get a sense of the dimensions of the data\n",
        "print(transactions.shape, '\\n')\n",
        "\n",
        "# get all the names of columns\n",
        "print(transactions.columns, '\\n')\n",
        "\n",
        "# get the attributes of all the columns\n",
        "print(transactions.dtypes, '\\n')\n",
        "\n",
        "# get a concise summary\n",
        "print(transactions.info(), '\\n')\n",
        "\n",
        "# get a count of missing values\n",
        "print(transactions.isnull().sum(), '\\n')\n",
        "\n",
        "# more inspection\n",
        "print(transactions['product_class'].unique(), '\\n')\n",
        "print(transactions['product_class'].nunique(), '\\n')\n",
        "\n",
        "# get descriptive statistics\n",
        "print(transactions.describe(), '\\n')"
      ],
      "metadata": {
        "id": "iCkASW5VNjH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data cleaning\n",
        "\n",
        "At this stage it makes sense to correct any issues or errors that are there in the data. Addressing data issues early can make sure important operations such as creating new columns or merging datasets produce expected results. Some of the most common data cleaning steps include -\n",
        "\n",
        "1. Addressing the presence of missing values\n",
        "2. Addressing the presence of duplicated information in the dataframe\n",
        "3. Renaming columns"
      ],
      "metadata": {
        "id": "5lEt3Uu1V2FM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dealing with missing values\n",
        "\n",
        "One of the most prevalent issues present in marketing datasets are missing values. These are present either due to lack of information for specific rows or due to errors in record keeping. Before attempting to manipuate data, it is crucial to come up with a strategy to deal with missing data."
      ],
      "metadata": {
        "id": "H7H0CS9pr0Pr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = {'A': [   1, 2, None, 4],\n",
        "        'B': [None, 5,    6, 7],\n",
        "        'C': [  10, 9,    1, 0]}\n",
        "df = pd.DataFrame(data)\n",
        "print(df,'\\n')\n",
        "\n",
        "# detect missing values\n",
        "print(df.isnull().sum(),'\\n')\n",
        "\n",
        "# total count\n",
        "print(df.isnull().sum().sum(),'\\n')\n",
        "\n",
        "# non null values\n",
        "print(df.notnull().sum(),'\\n')\n",
        "\n",
        "# removing rows with missing values for all columns\n",
        "print(df.dropna(axis = 0),'\\n')\n",
        "# removing columns that has any missing values\n",
        "print(df.dropna(axis = 1),'\\n')\n",
        "\n",
        "# removing rows with missing values for a specific column\n",
        "print(df.dropna(axis = 0, subset = [\"A\"]),'\\n')\n",
        "\n",
        "# removing columns that have a high proportion of missing values\n",
        "# df.dropna(axis = 1, thresh = len(df) * 0.8)\n",
        "\n",
        "# filling all missing values with 0 or with mean values\n",
        "print(df.fillna(0),'\\n')\n",
        "print(df.fillna(df.mean()),'\\n')\n",
        "\n",
        "# filling all missing values with the value before or after null\n",
        "print(df.fillna(method = 'ffill'),'\\n')\n",
        "print(df.fillna(method = 'bfill'),'\\n')\n",
        "\n",
        "# specific values for specific columns\n",
        "values = {'A': 2.0, 'B': 5.0}\n",
        "print(df.fillna(value = values),'\\n')\n",
        "\n",
        "# applying this to our dataset\n",
        "transactions_nonnull = transactions.dropna(axis = 0)\n",
        "print(transactions_nonnull.info(),'\\n')"
      ],
      "metadata": {
        "id": "BXmiOaYCV1o-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dealing with duplications\n",
        "\n",
        "Another common issue with data is the presence of duplicates. Sometimes duplicates are created because of inaccuracies in data recording or after merging data with another datasets. It is important to constantly check for the presence of duplicate rows because it can lead to inaccurate variables."
      ],
      "metadata": {
        "id": "5A9fa4sszomY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = {'A': [1, 2, 2, 3, 4],\n",
        "        'B': ['x', 'y', 'y', 'z', 'z']}\n",
        "df = pd.DataFrame(data)\n",
        "print(df,'\\n')\n",
        "\n",
        "# detect duplicated rows\n",
        "print(df.duplicated(),'\\n')\n",
        "\n",
        "# getting rid of duplicates\n",
        "print(df.drop_duplicates(),'\\n')\n",
        "\n",
        "# getting rid of duplicates but keeping the last one\n",
        "print(df.drop_duplicates(keep = \"last\"),'\\n')\n",
        "\n",
        "# getting rid of duplicates in a specific column\n",
        "print(df.drop_duplicates(subset = [\"B\"]),'\\n')"
      ],
      "metadata": {
        "id": "Js0NM-3xzq3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Renaming columns\n",
        "\n",
        "Data acquired from secondary sources can have column names that are long or difficult to deal with. Marketing datasets are no exception. It makes sense to properly rename columns at this stage if required."
      ],
      "metadata": {
        "id": "-RCs_5J4N4ZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transactions.rename(columns = {'transaction_date': 'date', 'standard_cost': 'scost'}).columns"
      ],
      "metadata": {
        "id": "j1yFjvJkN3vK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom classes for cleaning"
      ],
      "metadata": {
        "id": "i5MYgyse4AXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CleanRoutine:\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def remove_missing_values(self):\n",
        "        self.data = self.data.dropna()\n",
        "        return self.data\n",
        "\n",
        "    def remove_duplicates(self):\n",
        "        self.data = self.data.drop_duplicates()\n",
        "        return self.data\n",
        "\n",
        "    def rename_columns(self, column_mapping):\n",
        "        self.data = self.data.rename(columns = column_mapping)\n",
        "        return self.data\n",
        "\n",
        "data = {'First Name': ['Alice', 'Bob', None, 'David'],\n",
        "        'Last Name': ['Smith', 'Johnson', 'Brown', None],\n",
        "        'Age': [25, 30, 22, 28]}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(df)\n",
        "\n",
        "# create an instance of the DataCleaner class and perform cleaning operations\n",
        "# step by step\n",
        "routine = CleanRoutine(df)\n",
        "routine.remove_missing_values()\n",
        "routine.remove_duplicates()\n",
        "routine.rename_columns({'First Name': 'FirstName', 'Last Name': 'LastName'})\n",
        "clean_data = routine.data\n",
        "\n",
        "print(\"\\nCleaned DataFrame:\")\n",
        "print(clean_data)\n"
      ],
      "metadata": {
        "id": "gpl-20rG4Eu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Transformation\n",
        "\n",
        "At this stage the main task is to figure out how to change certain inherent properties of the dataset that would allow us to create new information or subset information appropriately."
      ],
      "metadata": {
        "id": "JlN-4CMpEomC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Changing the shape\n",
        "\n",
        "Often we need to change the shape of the dataframe to \"tidy\" up the data. Tidy data principles promote consistency and simplicity in data organization, making data analysis more efficient and less error-prone.\n",
        "\n",
        "**Each Variable Forms a Column:** Each variable should have its own column in the dataset. This makes it clear what each column represents and allows for easy manipulation.\n",
        "\n",
        "**Each Observation Forms a Row:** Each observation (data point) should have its own row in the dataset. This ensures that each piece of information is clearly associated with a specific observation.\n",
        "\n",
        "**Each Type of Observational Unit Forms a Table:** Each type of data should be stored in its own separate table. Related data should be organized together, and separate tables can be linked if needed."
      ],
      "metadata": {
        "id": "QbQVJFqN8Oxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example of data in wide format\n",
        "customer_info = pd.read_csv(\"https://raw.githubusercontent.com/pallavrouth/MarketingAnalytics/main/datasets/transaction_customerinfo.csv\")\n",
        "customer_info.head(n = 10)\n",
        "\n",
        "# changing the data from a wide to a long format\n",
        "customer_info_long = pd.melt(customer_info,\n",
        "                             id_vars = \"customer_id\",\n",
        "                             var_name = \"attribute\",\n",
        "                             value_name = \"value\")\n",
        "# print(customer_info_long.head(n = 10),'\\n')\n",
        "customer_info_long.sort_values(\"customer_id\").head(n = 10)"
      ],
      "metadata": {
        "id": "M_nimSrmGD-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data manipulation\n",
        "\n",
        "This is arguably the most important aspect of data wrangling. This step involves manipulating the columns and/or rows of the existing data frame to either subset data or reorganize data or generate new columns or summarize the data in ways that are crucial for testing hypothesis or answering research questions.\n",
        "\n"
      ],
      "metadata": {
        "id": "DagCNJ82NslN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Subsetting data\n",
        "\n",
        "We can subset data by (1) selecting certain columns and (2) by filtering rows that meet certain conditions"
      ],
      "metadata": {
        "id": "Wo0Epa972TGZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Selecting colums"
      ],
      "metadata": {
        "id": "BIAgi42E5wsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# selecting columns using '.loc' 'customer_id', 'transaction_date' 'product_id' 'list_price' 'order_status'\n",
        "# .loc[[row names]:[column names]]\n",
        "# for all rows use ':'\n",
        "transactions.loc[:,['transaction_date']].head() # note the difference between this and transactions.loc[:,'transaction_date'].head()\n",
        "# what products did a customer buy on a specific day and how much did they spend?\n",
        "transactions.loc[:,['customer_id','transaction_date','product_id','list_price','standard_cost']].head()\n",
        "# selecting a range of columns\n",
        "transactions.loc[:,'transaction_id':'transaction_date'].head() # notice that we do not supply a list\n",
        "# if I had to select the columns that had the word product how would I do it?\n",
        "columns = transactions.columns.to_list()\n",
        "mask = ['product' in col for col in columns]\n",
        "transactions.loc[:,mask]\n"
      ],
      "metadata": {
        "id": "nwW7UXiZOhwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Filtering rows"
      ],
      "metadata": {
        "id": "0AcN0Mk350yh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# when you want to filter based on column that has numerical values\n",
        "mask = transactions.list_price > 1000 # good thing is you dont need to convert list price to a list\n",
        "transactions.loc[mask] # transactions.loc[mask,:] for consistency\n",
        "transactions.loc[lambda d: d.list_price > 1000,:]\n",
        "\n",
        "# when you want to filter based on column that has categorical values\n",
        "transactions.loc[lambda d: d.product_class == 'medium',:]\n",
        "# better to use .isin()\n",
        "transactions.loc[lambda d: d.product_class.isin(['medium']),:]\n",
        "transactions.loc[lambda d: d.product_class.isin(['low','medium']),:]\n",
        "\n",
        "# multiple conditions\n",
        "# satisfies both conditions - and\n",
        "transactions.loc[lambda d: (d.list_price > 1000) & (d.product_class.isin(['low','medium'])),:]\n",
        "print(transactions.loc[lambda d: (d.list_price > 1000) & (d.product_class.isin(['low','medium'])),:].shape)\n",
        "# satisfies any one condition - or\n",
        "transactions.loc[lambda d: (d.list_price > 1000) | (d.product_class.isin(['low','medium'])),:]\n",
        "print(transactions.loc[lambda d: (d.list_price > 1000) | (d.product_class.isin(['low','medium'])),:].shape)"
      ],
      "metadata": {
        "id": "QFWs1Y3i53rM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reorganizing data\n",
        "\n",
        "We can reorganize data by sorting the data based on the values in one of the columns"
      ],
      "metadata": {
        "id": "2FBuIi6H_6G8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# what products did a customer buy on a specific day and how much did they spend?\n",
        "transactions_subset = transactions.loc[:,['customer_id','transaction_date','product_id','list_price','standard_cost']]\n",
        "transactions_organized = transactions_subset.sort_values(['customer_id'])\n",
        "transactions_organized_price = transactions_organized.sort_values(['list_price'], ascending = False)\n",
        "transactions_organized_price"
      ],
      "metadata": {
        "id": "k9ECvhGmADtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating new data or editing existing data\n",
        "\n",
        "Sometimes we need to generate new data by combining information from existing data in the dataframe. At other times we need to edit existing data."
      ],
      "metadata": {
        "id": "-VBsElCVAxXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(transactions_subset.transaction_date.to_list()[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUgqZiVIA6Vm",
        "outputId": "ddcced07-e51e-4674-8369-b2f09f1cdfce"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating new data"
      ],
      "metadata": {
        "id": "iOoItpvcDOtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# method 1\n",
        "transactions_subset_copy = transactions_subset.copy()\n",
        "transactions_subset_copy['list_price_100'] = transactions_subset['list_price'] / 100\n",
        "transactions_subset_copy\n",
        "\n",
        "# method 2 - preferred\n",
        "transactions_subset_copy2 = transactions_subset.copy()\n",
        "transactions_subset_copy2 = transactions_subset.assign(list_price_100 = lambda d: d.list_price / 100)\n",
        "transactions_subset_copy2\n",
        "\n",
        "transactions_subset\n",
        "\n",
        "# custom function to entire column\n",
        "import numpy as np\n",
        "def Max(x):\n",
        "  return np.nanmax(x)\n",
        "\n",
        "transactions_subset_copy = transactions_subset.assign(max_list_price = lambda d: Max(d.list_price))\n",
        "transactions_subset_copy\n",
        "\n",
        "# method 3 - when you need to apply a custom function to each row of a column use 'apply' (safer option)\n",
        "def Transform(x):\n",
        "  step1 = x/100\n",
        "  step2 = round(step1,2)\n",
        "  return f\"The transformed value is {step2}.\"\n",
        "\n",
        "# apply a function to each row of the\n",
        "transactions_subset_copy = transactions_subset.assign(max_list_price = lambda d: d.list_price.apply(Transform))\n",
        "transactions_subset_copy"
      ],
      "metadata": {
        "id": "-Hq5DHiqBpHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Special case - logic based data creation\n",
        "\n",
        "Sometimes we need to create new information based on whether it satisfies certain conditions. For example, indicator variables or dummy variables require using conditions or logic."
      ],
      "metadata": {
        "id": "1y9kOqfQT-Nh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transactions_subset_copy = transactions_subset.copy()\n",
        "# single condition\n",
        "transactions_subset_copy = transactions_subset.assign(ifelsecol = lambda d: np.where(d.list_price > 500,1,0))\n",
        "transactions_subset_copy\n",
        "\n",
        "# multiple conditions\n",
        "transactions_subset_copy = transactions_subset.assign(ifelsecol = lambda d: np.where((d.list_price > 500) & (d.list_price < 3000),1,0))\n",
        "transactions_subset_copy\n",
        "\n",
        "# matching conditions\n",
        "def match_condition(x):\n",
        "  if x > 0 and x <= 500: return \"cat1\"\n",
        "  elif x > 500 and x <= 1500: return \"cat2\"\n",
        "  else: return \"cat3\"\n",
        "\n",
        "transactions_subset_copy = transactions_subset.assign(cat_col = lambda d: d.list_price.apply(match_condition))\n",
        "transactions_subset_copy\n",
        "# transactions_subset_copy.cat_col.value_counts()"
      ],
      "metadata": {
        "id": "PKwAMhZ-UVta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Groupwise data creation\n",
        "\n",
        "Another popular special case of creating new data is when we have to edit information or create new data conditional on group membership. That is, we create create new variables using values within a group. This is true when the data has a heirarchical structure or has multiple level. This method is sometimes referred to split and apply."
      ],
      "metadata": {
        "id": "sXLoKkWqzTdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transactions_subset_copy = transactions_subset.copy()\n",
        "# what is the cumulative amount spent by each customer over time?\n",
        "transactions_subset_copy_sorted = transactions_subset.sort_values('customer_id')\n",
        "transactions_subset_copy_gd = transactions_subset_copy_sorted.assign(cumsum_price = lambda d: d.groupby('customer_id')['list_price'].transform('cumsum'))\n",
        "transactions_subset_copy_gd"
      ],
      "metadata": {
        "id": "yn1_-puozS9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Editing existing information\n",
        "\n",
        "Sometimes we need to edit existing information. This usually involves type conversion or re-casting existing data in certain formats. This type of data creation is often categorized under 'cleaning' step discussed previously."
      ],
      "metadata": {
        "id": "LPFxBUgXDRPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# converting strings to floats/numeric\n",
        "print(type(transactions_subset.standard_cost.to_list()[0]))\n",
        "float('10.45')\n",
        "# float('$10.45') # error\n",
        "def toFloat(x):\n",
        "  if isinstance(x, str):\n",
        "    float_str = x.replace('$','').replace(',','')\n",
        "    return float(float_str)\n",
        "  else:\n",
        "    return x\n",
        "\n",
        "transactions_subset_copy = transactions_subset.copy()\n",
        "transactions_subset_copy = transactions_subset.assign(fstandard_cost = lambda d: d.standard_cost.apply(toFloat))\n",
        "transactions_subset_copy\n",
        "\n",
        "# using astype in Pandas\n",
        "\n",
        "# converting strings to floats/numeric\n",
        "# print(type(transactions_subset.transaction_date.to_list()[0]))\n",
        "\n",
        "# frequent special case - taking care of date columns\n",
        "# transaction date is actually stored as string\n",
        "print(type(transactions_subset.transaction_date.to_list()[0]))\n",
        "# would be much better if it is stored as date type (which is unique to pandas)\n",
        "transactions_subset_copy = transactions_subset.copy()\n",
        "transactions_subset_copy = transactions_subset.assign(ftransaction_date = lambda d: pd.to_datetime(d.transaction_date))\n",
        "print(type(transactions_subset_copy.ftransaction_date.to_list()[0]))\n",
        "\n",
        "# why I prefer assign\n",
        "transactions_subset_copy = transactions_subset.assign(list_price_100 = lambda d: d.list_price / 100,\n",
        "                                                      ftransaction_date = lambda d: pd.to_datetime(d.transaction_date))\n",
        "transactions_subset_copy\n"
      ],
      "metadata": {
        "id": "Pk1YVuYGDckt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another popular special case of creating new data is when we have to edit information or create new data conditional on group membership. That is, we create create new variables using values within a group. This is true when the data has a heirarchical structure or has multiple level. This method is sometimes referred to split and apply."
      ],
      "metadata": {
        "id": "gA2REkLCyRw7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summarizing data\n",
        "\n",
        "Sometimes we need to summarize the information from multiple rows and columns. This is particularly useful when we need to analyze the data to understand important trends. We will go more into depth in the next class."
      ],
      "metadata": {
        "id": "veaJJHQ513Ig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using the tuple notation -- different agg functions to different columns\n",
        "transactions_subset.groupby('customer_id', as_index = False).agg(sum_spend = ('list_price','sum'),\n",
        "                                                                 avg_spend = ('list_price','mean'),\n",
        "                                                                 ntransactions = ('transaction_date','nunique'))\n",
        "\n",
        "# using the dictionary notation -- different agg functions to the same column\n",
        "aggregations = {\n",
        "    'list_price': {\n",
        "        'sum_spend': 'sum',\n",
        "        'avg_spend': 'mean',\n",
        "    },\n",
        "    'transaction_date': {\n",
        "        'ntransactions': 'nunique'\n",
        "    }\n",
        "}\n",
        "\n",
        "aggregations = {\n",
        "    'list_price': ['sum','mean'],\n",
        "    'transaction_date': 'nunique'\n",
        "}\n",
        "\n",
        "transactions_subset.groupby('customer_id', as_index = False).agg(aggregations)"
      ],
      "metadata": {
        "id": "lh9sAueMx6PP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Method chaining\n",
        "\n",
        "Method chaining in Pandas is a technique that involves combining multiple DataFrame or Series methods in a single statement, without the need to create intermediate variables. This approach allows you to perform a sequence of data manipulation operations more concisely and makes your code more readable.\n",
        "\n",
        "Method chaining is possible due to the fact that most Pandas methods return a modified copy of the data, which means you can immediately apply another method on the result, and so on."
      ],
      "metadata": {
        "id": "Exb3QLQt3v_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# step 1: select columns\n",
        "# step 2: arrange rows\n",
        "# step 3: filter rows\n",
        "# step 4: create new columns\n",
        "\n",
        "updated_data = (\n",
        "    transactions.loc[:,['customer_id','transaction_date','product_id','list_price','standard_cost']]\n",
        "      .sort_values(['customer_id'])\n",
        "      .loc[lambda d: d.list_price > 1000,:]\n",
        "      .assign(list_price_100 = lambda d: d.list_price / 100,\n",
        "              ftransaction_date = lambda d: pd.to_datetime(d.transaction_date))\n",
        ")\n",
        "\n",
        "updated_data.head()"
      ],
      "metadata": {
        "id": "DhDWZcnnjuqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation\n",
        "\n",
        "Data augmentation simply involves adding new data to the existing datasets. There are several key considerations when you consider merging multiple datasets.\n",
        "\n",
        "First, is what column would be used to merge the datasets. There must be a common column (also called 'keys') between the datasets before merging is attempted. Second, key consideration is how to should be merged. There are primary 4 important merging tactics - inner, outer, left and right. This [page](https://pandas.pydata.org/docs/dev/user_guide/merging.html) has helpful visuals on the principles behind these 4 types of merging process\n",
        "\n",
        "Data augmentation and data manipulation often goes hand in hand. Sometimes it makes sense to perform augmentation before manipulation. It really depends on what is more necessary."
      ],
      "metadata": {
        "id": "-a4vdE_H3s4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# finding common keys\n",
        "print(transactions.columns)\n",
        "print(transactions.shape)\n",
        "print(customer_info.columns)\n",
        "print(customer_info.shape)\n",
        "\n",
        "# how to merge -- left or inner are most common\n",
        "merged_data = transactions.merge(customer_info, how = 'left', on = 'customer_id')\n",
        "print(merged_data.columns)\n",
        "print(merged_data.shape)"
      ],
      "metadata": {
        "id": "2Os0lDsE38Oo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}