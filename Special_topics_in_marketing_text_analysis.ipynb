{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Amazon reviews dataset"
      ],
      "metadata": {
        "id": "C_y9Hjh7PTEf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ng9cxw0z0ikD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "review_data = pd.read_csv('amazon reviews.csv')"
      ],
      "metadata": {
        "id": "kqE-1Ff21KHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(review_data.columns)"
      ],
      "metadata": {
        "id": "JxCYRRcH18--",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33b3dbaf-b865-4b58-9752-941f2891bfa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Id',\n",
              " 'ProductId',\n",
              " 'UserId',\n",
              " 'ProfileName',\n",
              " 'HelpfulnessNumerator',\n",
              " 'HelpfulnessDenominator',\n",
              " 'Score',\n",
              " 'Time',\n",
              " 'Summary',\n",
              " 'Text']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "review_data_subset = (\n",
        "    review_data\n",
        "      .loc[:,['Id','UserId','ProfileName','Score','Text']]\n",
        "      .rename(columns = {'Id':'id',\n",
        "                         'UserId':'user_id',\n",
        "                         'ProfileName':'profile_name',\n",
        "                         'Score':'score',\n",
        "                         'Text':'text'})\n",
        ")\n",
        "\n",
        "\n",
        "review_data_subset"
      ],
      "metadata": {
        "id": "TKhkJqU22Ovr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Processing\n",
        "\n"
      ],
      "metadata": {
        "id": "odvbzgifPdjG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Preprocessing"
      ],
      "metadata": {
        "id": "e0abBUX3Pw0_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text preprocessing involves a series of tasks aimed at cleaning, transforming, and organizing text data to prepare it for analysis or natural language processing tasks. It helps in improving the quality of the input data and facilitates better performance of machine learning models or text-based algorithms."
      ],
      "metadata": {
        "id": "rJv6_PHd6M5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Apple is looking at buying U.K. startup for $1 billion.\"\n",
        "print(text)"
      ],
      "metadata": {
        "id": "D_eJST632_df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86a9323f-e1c8-49da-c23b-484086af2159"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple is looking at buying U.K. startup for $1 billion.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lower_text = text.lower()\n",
        "print(lower_text)"
      ],
      "metadata": {
        "id": "LqeJu5c8BH9S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d061bc1c-fc28-4c6f-8360-798ca1fd8a7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "apple is looking at buying u.k. startup for $1 billion.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokens and tokenization"
      ],
      "metadata": {
        "id": "NIKiB6C2P6w7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In computing and linguistics, \"tokens\" refer to the individual units that make up a larger body of text or data. Tokenization is the process of breaking down a stream of text into smaller units, which can be words, phrases, symbols, or other meaningful elements. These units, or tokens, serve as the fundamental building blocks for various natural language processing tasks, such as machine learning, text analysis, and linguistic analysis.\n",
        "\n",
        "In natural language processing (NLP), tokenization involves splitting a piece of text into tokens, which can be individual words, subwords, characters, or even larger units like phrases or sentences. The tokens created through this process are used as inputs for tasks like text classification, language modeling, sentiment analysis, and more.\n",
        "\n",
        "For instance, consider the sentence: \"The quick brown fox jumps over the lazy dog.\" Tokenizing this sentence might result in tokens like: [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\", \".\"]. These tokens can then be used for further analysis or processing by an algorithm or system."
      ],
      "metadata": {
        "id": "-wDNjjqhbo9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(lower_text)\n",
        "\n",
        "tokens = []\n",
        "for token in doc:\n",
        "    tokens.append(token.text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "tsPqNn-aBGmo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73e8ecf1-c44f-496a-b955-2b133647ee86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['apple', 'is', 'looking', 'at', 'buying', 'u.k', '.', 'startup', 'for', '$', '1', 'billion', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "  if not token.is_stop:\n",
        "    print(token.text)"
      ],
      "metadata": {
        "id": "LLC7uW0f4zYd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7178dcc4-bf06-4cc7-933b-be67a4b85642"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "apple\n",
            "looking\n",
            "buying\n",
            "u.k\n",
            ".\n",
            "startup\n",
            "$\n",
            "1\n",
            "billion\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "  if not token.is_punct:\n",
        "    print(token.text)"
      ],
      "metadata": {
        "id": "qojWJwwLmCrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "for token in doc:\n",
        "  if not re.match('[^A-Za-z]+', token.text):\n",
        "    print(token.text)"
      ],
      "metadata": {
        "id": "DNLh7aMMmOLY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e0cffe0-71c3-4b11-f570-de06046af2e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "apple\n",
            "is\n",
            "looking\n",
            "at\n",
            "buying\n",
            "u.k\n",
            "startup\n",
            "for\n",
            "billion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_tokens = []\n",
        "for token in doc:\n",
        "  if not (token.is_stop or token.is_punct or re.match('[^A-Za-z]+', token.text)):\n",
        "    filtered_tokens.append(token.text)\n",
        "\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "id": "YV30E0q-l2e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3339b37c-5b0b-4010-d05e-0b9f84dc12cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['apple', 'looking', 'buying', 'u.k', 'startup', 'billion']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization"
      ],
      "metadata": {
        "id": "_ka6YzJG6Bd_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization is the process of reducing words to their base or canonical form, known as the lemma, considering their morphological analysis and the context in which they appear. The goal of lemmatization is to group together inflected forms of a word to analyze them as a single item, referred to as the lemma or dictionary form."
      ],
      "metadata": {
        "id": "XFj_cw3n6bpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_token_lemmas = []\n",
        "for token in doc:\n",
        "  if not (token.is_stop or token.is_punct or re.match('[^A-Za-z]+', token.text)):\n",
        "    filtered_token_lemmas.append(token.lemma_)\n",
        "\n",
        "print(filtered_token_lemmas)"
      ],
      "metadata": {
        "id": "oe2EF7B5B7ls",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22078749-0bf6-4ba3-90e7-e8e289dab7af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['apple', 'look', 'buy', 'u.k', 'startup', 'billion']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed_text = ' '.join(filtered_token_lemmas)\n",
        "\n",
        "print(text)\n",
        "print(processed_text)"
      ],
      "metadata": {
        "id": "84mWUSrLCEPz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "691477fd-abb3-47ef-e798-42a3980a5909"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple is looking at buying U.K. startup for $1 billion.\n",
            "apple look buy u.k startup billion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(input_text):\n",
        "  lower_text = input_text.lower()\n",
        "  doc = nlp(lower_text)\n",
        "  filtered_tokens = []\n",
        "  for token in doc:\n",
        "    if not (token.is_stop or token.is_punct or re.match('[^A-Za-z]+', token.text)):\n",
        "      filtered_tokens.append(token.lemma_)\n",
        "  out_text = ' '.join(filtered_tokens).strip()\n",
        "  return out_text"
      ],
      "metadata": {
        "id": "gh0wzcL2D5Uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "sample_document = review_data_subset.text.to_list()[5]\n",
        "pprint(sample_document)"
      ],
      "metadata": {
        "id": "yFVu02IMEcGM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "912e411c-9ee7-42df-aaee-b860d7e9ed9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('I got a wild hair for taffy and ordered this five pound bag. The taffy was '\n",
            " 'all very enjoyable with many flavors: watermelon, root beer, melon, '\n",
            " 'peppermint, grape, etc. My only complaint is there was a bit too much '\n",
            " 'red/black licorice-flavored pieces (just not my particular favorites). '\n",
            " 'Between me, my kids, and my husband, this lasted only two weeks! I would '\n",
            " 'recommend this brand of taffy -- it was a delightful treat.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(preprocess(input_text = sample_document))"
      ],
      "metadata": {
        "id": "SQa8lgbXEtcM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d1de3b8-0352-46c0-beda-1464f9736d93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('get wild hair taffy order pound bag taffy enjoyable flavor watermelon root '\n",
            " 'beer melon peppermint grape etc complaint bit red black licorice flavor '\n",
            " 'piece particular favorite kid husband last week recommend brand taffy '\n",
            " 'delightful treat')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Document and Corpus"
      ],
      "metadata": {
        "id": "CRSE-EiXQPBI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the context of natural language processing (NLP) and text analysis, a \"document\" refers to a unit of text that could range from a single sentence to a complete piece of writing, such as an article, a book chapter, an email, or any other identifiable textual unit. Essentially, a document is any piece of text that can be considered as a standalone entity for analysis.\n",
        "\n",
        "On the other hand, a \"corpus\" refers to a collection of documents or texts that are used for linguistic analysis, machine learning, or other NLP tasks. A corpus can contain a few documents or millions of them, depending on the scope and purpose of the analysis. Corpora (plural of corpus) are used extensively in computational linguistics and NLP to train models, study linguistic patterns, develop algorithms, and perform various text-based research and analysis."
      ],
      "metadata": {
        "id": "U6ZXUL20cNVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review_corpus = review_data_subset.text.to_list()[0:2000]\n",
        "#pprint(review_corpus)"
      ],
      "metadata": {
        "id": "Uw-TC9WfE3Ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_corpus"
      ],
      "metadata": {
        "id": "Pm344SMDp3Gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_corpus_processed = []\n",
        "for document in review_corpus:\n",
        "  review_corpus_processed.append(preprocess(input_text = document))\n",
        "\n",
        "pprint(review_corpus_processed)"
      ],
      "metadata": {
        "id": "3Pwr5KqxF07a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Representation"
      ],
      "metadata": {
        "id": "s-xmwlXnQewz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bag of words (BOW)"
      ],
      "metadata": {
        "id": "eruK9wGTQhaF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Bag of Words (BoW) representation is a simple and commonly used technique in natural language processing for converting text data into numerical vectors. It's a way to represent text data quantitatively, disregarding grammar and word order, and focusing solely on the presence and frequency of words in a document.\n",
        "\n",
        "Here's how the Bag of Words representation typically works:\n",
        "\n",
        "1. Vocabulary Creation: First, a vocabulary is created by collecting unique words from the entire corpus of documents. Each unique word is a \"token\" in the vocabulary.\n",
        "\n",
        "2. Vectorization: For each document in the corpus, a vector is created where each element represents the count (or presence) of a word from the vocabulary in that specific document.\n",
        "\n",
        "BoW representations are simple and easy to implement but have limitations:\n",
        "\n",
        "Lose context: They ignore the order of words, losing information about the sequence of words in a document.\n",
        "High-dimensional vectors: In large vocabularies or datasets, BoW representations result in high-dimensional sparse vectors, which can be computationally expensive and memory-intensive."
      ],
      "metadata": {
        "id": "GZ3DPWrf1sVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "corpus = [\n",
        "    \"This is the first document.\",\n",
        "    \"This document is the second document.\",\n",
        "    \"And this is the third one.\",\n",
        "    \"Is this the first document?\",\n",
        "]\n",
        "\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "bow_representation = vectorizer.fit_transform(corpus)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "list(feature_names)"
      ],
      "metadata": {
        "id": "Sg6pAaJyFiDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow_array = bow_representation.toarray()\n",
        "\n",
        "print(\"Feature Names (Vocabulary):\", feature_names)\n",
        "print(\"Bag of Words Representation:\")\n",
        "print(bow_array)"
      ],
      "metadata": {
        "id": "WR55sBJ2rqu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow_df = pd.DataFrame(bow_array, columns = feature_names)\n",
        "bow_df"
      ],
      "metadata": {
        "id": "iqAsSlIKr7en"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow_representation = vectorizer.fit_transform(review_corpus_processed)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "bow_array = bow_representation.toarray()\n",
        "\n",
        "bow_df = pd.DataFrame(bow_array, columns = feature_names)\n",
        "bow_df"
      ],
      "metadata": {
        "id": "omlHbqpJGFem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF Matrix"
      ],
      "metadata": {
        "id": "0WpIx8rdQlyd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF stands for Term Frequency-Inverse Document Frequency, which is a numerical statistic used in natural language processing to evaluate the importance of a word in a document relative to a collection of documents (a corpus).\n",
        "\n",
        "TF-IDF representation aims to reflect how important a word is to a document in a collection by considering two factors:\n",
        "\n",
        "Term Frequency (TF): This measures how frequently a term (word) appears in a document. It is calculated as the number of times a term occurs in a document divided by the total number of terms in that document. The idea is that the more often a word appears in a document, the more important or relevant it might be to that document.\n",
        "\n",
        "Inverse Document Frequency (IDF): IDF measures how unique or rare a term is across the entire corpus. It is calculated as the logarithm of the total number of documents divided by the number of documents containing the term. The rarer the term (i.e., the fewer documents it appears in), the higher its IDF value.\n",
        "\n",
        "The goal of using TF-IDF is to assign weights to words in a document based on their relevance to that document and their uniqueness across the entire corpus. Words that are common across many documents (like \"the,\" \"is,\" etc.) tend to have lower TF-IDF scores because they are less informative, while words that are unique to specific documents and carry more meaning receive higher scores.\n",
        "\n",
        "Terms with higher TF-IDF scores are considered more important or relevant to a particular document because they occur frequently within that document but are not widely spread across other documents in the corpus."
      ],
      "metadata": {
        "id": "hJEknGJe3L9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_representation = vectorizer.fit_transform(review_corpus_processed)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "tfidf_array = tfidf_representation.toarray()\n",
        "\n",
        "tfidf_df = pd.DataFrame(tfidf_array, columns = feature_names)\n",
        "tfidf_df"
      ],
      "metadata": {
        "id": "oqSFHXTGGgMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings"
      ],
      "metadata": {
        "id": "ywWX2ttZQqvI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding representations are dense, low-dimensional numerical representations of words, phrases, or entities in a continuous vector space. They are created using techniques that map high-dimensional, sparse, and discrete representations (like words represented by one-hot vectors or indices) into a lower-dimensional space where semantically similar entities are closer together.\n",
        "\n",
        "Word embeddings, for instance, capture the semantic relationships between words by placing them in a multi-dimensional space where words with similar meanings or contexts are located nearer to each other"
      ],
      "metadata": {
        "id": "m2bzRsWf4JUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "metadata": {
        "id": "5yQ_D1xnM2x2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "id": "APEuk1rwLbuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "embeddings_dict = {}\n",
        "\n",
        "for document in review_corpus_processed:\n",
        "    document_embedding = embedder.encode(document, convert_to_tensor=True)\n",
        "    embeddings_dict[document] = document_embedding.numpy()\n",
        "\n",
        "embeddings_df = pd.DataFrame(embeddings_dict.items(), columns = ['sentence','embeddings'])"
      ],
      "metadata": {
        "id": "VYpuUFMnNE_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_df.embeddings[0]\n",
        "embeddings_df.embeddings[0].shape"
      ],
      "metadata": {
        "id": "Eosw5x_xpFgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Mining using BOW\n"
      ],
      "metadata": {
        "id": "IKsxnJqPQ1q1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review_corpus_1k = review_data_subset.text.to_list()[0:1000]\n",
        "\n",
        "review_corpus_processed = []\n",
        "for document in review_corpus_1k:\n",
        "  review_corpus_processed.append(preprocess(input_text = document))"
      ],
      "metadata": {
        "id": "k6wW3-FLqTNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unigram bigram analysis"
      ],
      "metadata": {
        "id": "7r5fAWUg356H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(ngram_range = (1, 1), min_df = 2)\n",
        "bow_representation = vectorizer.fit_transform(review_corpus_processed)\n",
        "feature_names = vectorizer.get_feature_names_out()"
      ],
      "metadata": {
        "id": "J9vR51Jsy154"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow_array = bow_representation.toarray()\n",
        "bow_df = pd.DataFrame(bow_array, columns = feature_names)\n",
        "bow_df"
      ],
      "metadata": {
        "id": "C3pfTu86sXH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow_df.sum(axis = 0)"
      ],
      "metadata": {
        "id": "epK6GRW-yqFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame({'frequency' : bow_df.sum(axis = 0)}).reset_index().rename(columns = {'index':'word'})"
      ],
      "metadata": {
        "id": "BHnALuHEzAk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "occurrences_df = (\n",
        "    pd.DataFrame({'frequency' : bow_df.sum(axis = 0)})\n",
        "      .reset_index()\n",
        "      .rename(columns = {'index':'word'})\n",
        ")"
      ],
      "metadata": {
        "id": "rDfvsNnfzT4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "occurrences_df.sort_values('frequency', ascending = False)"
      ],
      "metadata": {
        "id": "JyMi2-dX0P95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(ngram_range = (2, 2), min_df = 2)\n",
        "bow_representation = vectorizer.fit_transform(review_corpus_processed)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "bow_array = bow_representation.toarray()\n",
        "bow_df = pd.DataFrame(bow_array, columns = feature_names)\n",
        "\n",
        "occurrences_df = (\n",
        "    pd.DataFrame({'frequency' : bow_df.sum(axis = 0)})\n",
        "      .reset_index()\n",
        "      .rename(columns = {'index':'word'})\n",
        ")\n",
        "\n",
        "occurrences_df.sort_values('frequency', ascending = False)"
      ],
      "metadata": {
        "id": "xN4BwUlF3o6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word Cloud"
      ],
      "metadata": {
        "id": "ri7QZKZjRAIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "combined_text = ' '.join(review_corpus_processed)\n",
        "wordcloud = WordCloud(width = 800, height = 400, background_color = 'white').generate(combined_text)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Word Cloud from review_corpus_lower Sentences')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "05OIc4OexPNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Modeling"
      ],
      "metadata": {
        "id": "yRmWXvNuRNlk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Topic Modeling"
      ],
      "metadata": {
        "id": "C2dJ99MwRQWN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Topic modeling is a technique used in natural language processing (NLP) to discover the topics or themes present in a collection of texts. It's a way to automatically identify the hidden patterns in a large corpus of documents and organize them based on the recurring themes they contain.\n",
        "\n",
        "Topic modeling finds applications in various fields due to its ability to extract underlying themes and structures from text data. In marketing they can useful when\n",
        "\n",
        "1. **Content Recommendation:** It powers recommendation systems by identifying topics of interest based on user preferences and suggesting relevant content.\n",
        "\n",
        "2. **Market Research and Social Media Analysis:** Analyzing social media posts, reviews, or customer feedback to understand trends, sentiments, and topics of discussion within specific domains.\n",
        "\n",
        "3. **Customer Support and Feedback Analysis:** Analyzing customer support tickets, surveys, or feedback to identify recurring issues or topics of concern."
      ],
      "metadata": {
        "id": "UGTXHFSWgM48"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LDA\n",
        "\n",
        "Latent Dirichlet Allocation (LDA) is a generative probabilistic model used for topic modeling. It's based on the idea that documents are represented as random mixtures of latent topics, and each topic is characterized by a distribution of words.\n",
        "\n",
        "LDA uses statistical inference techniques to reverse-engineer this process: given a collection of documents, it attempts to find the topics that best explain the observed word co-occurrence.\n",
        "It iterates through each word in each document and tries to adjust the probabilities of topics and words to find a set of topics that best represents the entire document collection.\n",
        "\n",
        "After the model has been trained, it provides two main outputs:\n",
        "- The distribution of topics across the documents.\n",
        "- The distribution of words across the topics."
      ],
      "metadata": {
        "id": "Rqe0o_vNg73p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim import corpora\n",
        "from gensim import models"
      ],
      "metadata": {
        "id": "3aGG9Pl_GWa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_documents = []\n",
        "for document in review_corpus_processed:\n",
        "    doc = nlp(document)\n",
        "    tokens = []\n",
        "    for token in doc:\n",
        "      tokens.append(token.text)\n",
        "    tokenized_documents.append(tokens)"
      ],
      "metadata": {
        "id": "MTAMRG5o43_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(tokenized_documents[0:5])"
      ],
      "metadata": {
        "id": "uwpCRDdX6ZOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = corpora.Dictionary(tokenized_documents)\n",
        "dictionary.filter_extremes(no_below = 2, no_above = 0.99)"
      ],
      "metadata": {
        "id": "PDkjhRr04EvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dictionary object contains the vocabulary of terms present in the collection of documents after filtering out terms that are too rare (appear in fewer than 2 documents) or too common (appear in more than 99% of the documents). For example, every tokenized word in the dictionary has a code associated with it."
      ],
      "metadata": {
        "id": "MU1MRGG_h0TL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token_id, token in dictionary.items():\n",
        "    print(token_id, token)"
      ],
      "metadata": {
        "id": "lWS5nm466t0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = []\n",
        "for document in tokenized_documents:\n",
        "  corpus.append(dictionary.doc2bow(document))"
      ],
      "metadata": {
        "id": "wD24OuJ06rO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting the dictionary into a bag of words."
      ],
      "metadata": {
        "id": "B9iz5-h2h5c-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus[0]"
      ],
      "metadata": {
        "id": "C5ISDuXA8j24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Number of unique tokens: %d' % len(dictionary))\n",
        "print('Number of documents: %d' % len(corpus))"
      ],
      "metadata": {
        "id": "hbIkGYPRCsDs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa5295ba-1352-4a20-941c-0d9386442392"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique tokens: 3310\n",
            "Number of documents: 2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lda = models.LdaModel(corpus, num_topics = 4, alpha = 'auto', eta = 'auto',\n",
        "                      iterations = 100, eval_every = None,\n",
        "                      id2word = dictionary, passes = 30)"
      ],
      "metadata": {
        "id": "BrV4OS_kCupN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Topics and their constituent words:\")\n",
        "for topic_id, topic in lda.print_topics():\n",
        "    print(f\"Topic {topic_id}: {topic}\")"
      ],
      "metadata": {
        "id": "R52iYZp9-NCJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4caeb9e-0d6e-42f4-e09a-e373b9413667"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topics and their constituent words:\n",
            "Topic 0: 0.017*\"sugar\" + 0.016*\"product\" + 0.015*\"tea\" + 0.012*\"br\" + 0.012*\"taste\" + 0.012*\"like\" + 0.009*\"good\" + 0.008*\"use\" + 0.008*\"drink\" + 0.008*\"try\"\n",
            "Topic 1: 0.026*\"food\" + 0.017*\"dog\" + 0.012*\"love\" + 0.012*\"like\" + 0.012*\"good\" + 0.011*\"eat\" + 0.010*\"find\" + 0.009*\"taste\" + 0.008*\"great\" + 0.008*\"buy\"\n",
            "Topic 2: 0.019*\"good\" + 0.015*\"like\" + 0.013*\"product\" + 0.012*\"taste\" + 0.010*\"flavor\" + 0.010*\"love\" + 0.010*\"coffee\" + 0.010*\"order\" + 0.009*\"tea\" + 0.009*\"use\"\n",
            "Topic 3: 0.055*\"chip\" + 0.028*\"flavor\" + 0.021*\"bag\" + 0.018*\"good\" + 0.017*\"like\" + 0.015*\"taste\" + 0.014*\"great\" + 0.013*\"love\" + 0.013*\"salt\" + 0.012*\"potato\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentiment Analysis"
      ],
      "metadata": {
        "id": "IifO8xAYRZaK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentiment analysis is a natural language processing (NLP) technique used to determine the sentiment expressed in a piece of text. It involves using computational methods to analyze and identify the subjective information present in the text, usually to understand the attitude, opinion, or emotion conveyed by the writer or speaker.\n",
        "\n",
        "The primary goal of sentiment analysis is to classify the sentiment of the text as positive, negative, or neutral, and sometimes it might involve more nuanced classification like detecting emotions (such as joy, anger, sadness, etc.).\n",
        "\n",
        "#### Lexicon based sentiment analysis\n",
        "\n",
        "A lexicon, in the context of natural language processing (NLP), refers to a dictionary or collection of words, phrases, or entities with associated information such as their meanings, parts of speech, or sentiment polarities.\n",
        "\n",
        "Lexicons are used extensively in language processing tasks to assist in tasks like sentiment analysis, where the lexicon contains information about the sentiment or polarity of words. Lexicon-based sentiment analysis, therefore, refers to a technique that determines the sentiment of a piece of text by looking up the sentiment of individual words or phrases in a predefined lexicon.\n",
        "\n",
        "A lexicon is created or compiled containing words or phrases mapped to their corresponding sentiment polarities. Each word or phrase in the lexicon is associated with a sentiment score (e.g., positive, negative, or neutral) or a numerical value indicating sentiment strength."
      ],
      "metadata": {
        "id": "DDGxAdvGieU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(review_corpus[0])"
      ],
      "metadata": {
        "id": "4uowUDCS-cWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "id": "BOOz-C68-9cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_list = [\"I love this product, it's amazing!\",\n",
        "             \"The service was terrible, very disappointing.\",\n",
        "             \"The movie was okay, not great though.\"]\n",
        "\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "for text in text_list:\n",
        "    sentiment_scores = sid.polarity_scores(text)\n",
        "    print(f\"Text: '{text}'\")\n",
        "    print(f\"Sentiment Scores: {sentiment_scores}\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "4XDD3s92_B4Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b571782b-7883-4f6a-cac8-ab4f1b62cbef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: 'I love this product, it's amazing!'\n",
            "Sentiment Scores: {'neg': 0.0, 'neu': 0.266, 'pos': 0.734, 'compound': 0.8516}\n",
            "\n",
            "\n",
            "Text: 'The service was terrible, very disappointing.'\n",
            "Sentiment Scores: {'neg': 0.622, 'neu': 0.378, 'pos': 0.0, 'compound': -0.7645}\n",
            "\n",
            "\n",
            "Text: 'The movie was okay, not great though.'\n",
            "Sentiment Scores: {'neg': 0.323, 'neu': 0.49, 'pos': 0.186, 'compound': -0.3387}\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for text in text_list:\n",
        "    sentiment_scores = sid.polarity_scores(text)\n",
        "    if sentiment_scores['compound'] >= 0.05: sentiment = 'Positive'\n",
        "    elif sentiment_scores['compound'] <= -0.05: sentiment = 'Negative'\n",
        "    else: sentiment = 'Neutral'\n",
        "\n",
        "    print(f\"Text: '{text}'\")\n",
        "    print(f\"Sentiment: {sentiment}\")\n",
        "    print(f\"Sentiment Scores: {sentiment_scores}\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "3hnTkGyO_Xdx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b67f1106-59e5-4a26-aa1d-4d817d4d3b4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: 'I love this product, it's amazing!'\n",
            "Sentiment: Positive\n",
            "Sentiment Scores: {'neg': 0.0, 'neu': 0.266, 'pos': 0.734, 'compound': 0.8516}\n",
            "\n",
            "\n",
            "Text: 'The service was terrible, very disappointing.'\n",
            "Sentiment: Negative\n",
            "Sentiment Scores: {'neg': 0.622, 'neu': 0.378, 'pos': 0.0, 'compound': -0.7645}\n",
            "\n",
            "\n",
            "Text: 'The movie was okay, not great though.'\n",
            "Sentiment: Negative\n",
            "Sentiment Scores: {'neg': 0.323, 'neu': 0.49, 'pos': 0.186, 'compound': -0.3387}\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentiment(input_text):\n",
        "  sentiment_scores = sid.polarity_scores(input_text)\n",
        "  return sentiment_scores['compound']"
      ],
      "metadata": {
        "id": "dxr8z98oARJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  if sentiment_scores['compound'] >= 0.05: sentiment = 'Positive'\n",
        "  elif sentiment_scores['compound'] <= -0.05: sentiment = 'Negative'\n",
        "  else: sentiment = 'Neutral'"
      ],
      "metadata": {
        "id": "iIGBbQ3OIfqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_data_subset_1k = review_data_subset.iloc[0:1000,]\n",
        "review_data_subset_1k"
      ],
      "metadata": {
        "id": "YrQl6MJuA3j0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_data_subset_1k.assign(score = lambda d: d.text.apply(get_sentiment),\n",
        "                             sentiment = lambda d: np.where(d.score > 0.05,\"positive\",\n",
        "                                                              np.where(d.score < - 0.05,\"negative\",\"neutral\")))"
      ],
      "metadata": {
        "id": "LN8DrMrqIInB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Semantic Search"
      ],
      "metadata": {
        "id": "2eTQzS5NJW54"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Semantic\" relates to the meaning or interpretation of language, symbols, or signs within a particular context. It refers to the study of meaning in language, focusing on how words, phrases, symbols, or elements convey information, concepts, or ideas.\n",
        "\n",
        "In essence, semantics deals with the understanding and interpretation of the significance, relationships, and associations between different linguistic elements, such as words, phrases, sentences, or symbols, and how they convey meaning within a given context.\n",
        "\n",
        "Semantic search in natural language processing (NLP) refers to a search technique that aims to improve the accuracy and relevance of search results by understanding the intent and context behind the user's query rather than relying solely on keyword matching. It focuses on the meaning (semantics) of words and the relationship between different concepts within a query and the searched content."
      ],
      "metadata": {
        "id": "7rVLM01eoLjB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Semantic Similarity\n",
        "\n",
        "Techniques such as word embeddings or vector representations are often employed to measure the semantic similarity between the query and the content in the database. These methods map words or phrases into high-dimensional vectors, enabling calculations of similarity based on their positions in the vector space."
      ],
      "metadata": {
        "id": "k1p-FRWooZNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "id": "-o_CPkESocOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_embeddings = embedder.encode(review_corpus, convert_to_tensor = True)"
      ],
      "metadata": {
        "id": "vzo8mAsKqBnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query sentences:\n",
        "queries = ['would not recommend']\n",
        "\n",
        "# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\n",
        "top_k = min(5, len(review_corpus))\n",
        "for query in queries:\n",
        "    query_embedding = embedder.encode(query, convert_to_tensor = True)\n",
        "\n",
        "    # We use cosine-similarity and torch.topk to find the highest 5 scores\n",
        "    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
        "    top_results = torch.topk(cos_scores, k = top_k)\n",
        "\n",
        "    print(\"\\n\\n======================\\n\\n\")\n",
        "    print(\"Query:\", query)\n",
        "    print(\"\\nTop 5 most similar sentences in corpus:\")\n",
        "\n",
        "    for score, idx in zip(top_results[0], top_results[1]):\n",
        "        print(review_corpus[idx], \"(Score: {:.4f})\".format(score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9C-GgPO3pyno",
        "outputId": "9907d0a1-9a22-42ff-a305-e82ecc4bd3f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "Query: would not recommend\n",
            "\n",
            "Top 5 most similar sentences in corpus:\n",
            "It is okay.  I would not go out of my way to buy it again (Score: 0.3513)\n",
            "This offer is a great price and a great taste, thanks Amazon for selling this product.<br /><br />Staral (Score: 0.3315)\n",
            "These are super tastey! I would definitely recommend. The only reason I'm not giving 5 stars is because I wish they were bigger! :D (Score: 0.3257)\n",
            "Second order.  Very good, hot which I like but best to sample as per your preferences. (Score: 0.3216)\n",
            "thank you for this product - we use it all the time and appreciate your promptness and the price was excellent.  Thanks again. (Score: 0.3091)\n"
          ]
        }
      ]
    }
  ]
}