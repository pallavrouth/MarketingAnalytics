{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNqOWpgbd6dr6ef9gv0Za5V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pallavrouth/MarketingAnalytics/blob/main/Predictive_modeling_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Core Constructs in Statistical Learning"
      ],
      "metadata": {
        "id": "jUw8-KqwbY8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recap: Prediction vs Inference\n",
        "\n",
        "Prediction involves using a statistical model to make forecasts or estimates about future or unseen data points. It is typically used when the goal is to make informed guesses or projections about what might happen. It is generally used to answer questions such as -\n",
        "\n",
        "1. Can we predict a response given some predictors?\n",
        "2. How can we accurately make a prediction of a response given some predictors?\n",
        "\n",
        "Inference involves drawing conclusions or making inferences about a population or a process based on a sample of data. It is used to understand the associations between variables. Inference is typically used to test hypothesis testing related to these associations. It is used to answer questions such as -\n",
        "\n",
        "1. Which predictors are associated with the response?\n",
        "2. What is the nature relationship between the response and each predictor?"
      ],
      "metadata": {
        "id": "FtXqi6helDCq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction Performance\n",
        "\n",
        "Prediction is the ultimate goal of machine learning models. In simple words, the task for all **supervised ML** models to **generate outputs given a set of inputs**. For example, a bank wants to predict whether a customer will stay with the firm given his or her transaction history. One can build a machine learning model that will predict a customer's staying behavior (the output) given his or her transaction history (the inputs). At this point, it is important for an analyst to know whether the prediction model he made is accurate. Because the bank may decide to take (costly) measures to keep the customer.\n",
        "\n",
        "Therefore, a fundamental task in the machine learning pipeline is to **evaluate the quality of the ML model**. One way to evaluate the quality is to check how well the model can perform on on **new or unseen data**. The better the performance the higher confidence of the analyst in using the model in the future.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lehCuslgeK5e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating Models - The ML workflow\n",
        "\n",
        "Notice I say, in order to assess or evaluating the quality of an ML model, I need new or unseen data. This is data that will be generated in the future (say tomorrow). This raises an important question - **how does an analyst test the performance a machine learning model today if the unseen data will manifest tomorrow?**\n",
        "\n",
        "The answer is that **we pretend** that certain fraction of the data **chosen randomly**, is new or unseen data. The basic logic is that because we are choosing this \"pretend data\" randomly from the current available dataset today, it is representative of the data that will be generated tomorrow. Therefore, given a dataset, the analysts task is to **divide it** randomly into **training** data and **testing data**. They are called so because the analyst can build a machine learning model on the training data and then evaluate how well the same model is doing on the test data.\n",
        "\n",
        "But, exactly what steps encompass \"building\" and \"evaluate\" models? Building a model simply means **feeding** necessary set of input variables (or independent variables or features) and dependent variable (or the target variable) into a predictiive ML algorithm to create a predictive ML model. This step is also referred to as **fitting the model** or **training the model**. Remember we always use the training data to fit or train a model.\n",
        "\n",
        "The ML algorithm is capable of learning how certain patterns in the inputs corresponds to a certain output in the training data. More importantly, this learning capability of predictive ML models means that **you can ask the model to generate predictions of the dependent variable given a new or unseen set of independent variables**. Specifically, you can use the input variables in the testing data to ask the model to create predictions of the dependent or target variable. Evaluating the quality of the model then involves using these predictions and **comparing them to the actual values** of dependent variable.\n",
        "\n",
        "\n",
        "Going back to our example of predicting a customer's propensity to stay with the bank, how does this process of model building and evaluation on training and testing data work? We know that the analyst working for the bank uses the transaction history of customers till date as input variables or independent  (let's call the collection of these variables **X** for simplicity) and the customer's decision to stay or not as dependent or target variable (let's call this variable **Y** for simplicity) . Given this setup, the analyst splits the data (both **X** and **Y**) randomly into a training data and testing data. Therefore, the training data consists of **training X** and **training Y** from a random fraction of customers. And, the testing data consists of **testing X** and **testing Y** from the remaining random fraction of customers.\n",
        "\n",
        "The analyst then builds an ML model of his or her choice using the **training X** and **training Y**. She then uses this trained ML model on her **testing X** to create predictions of **Y**. Recall, **Y** is the customer's decision to stay or not. Therefore, **testing Y** is actual data on customer's decision. Thus, by comparing the predictions to **testing Y**, the analyst is able to compare how well the predictions hold up against actual observations.\n",
        "\n",
        "More formally, we can organize the analysts workflow into the following steps -\n",
        "\n",
        "1. **Feature engineering** - This is when the analyst does her own research to figure out what set of input variables are relevant to the output. If needed she uses the data to create some measures. This step is crucial because the quality of the ML model depend on how relevant your independent variables are.\n",
        "\n",
        "2. **Split the data** - **Randomly** divide the records in the dataset into a **training** set and a **testing** set.\n",
        "\n",
        "3. **Picking a suitable model** - Next, the analyst choses an ML model that is suited for this task. Depending on the nature of the dependent or target variable, the analyst can pick from a plethora of classification versus regression type ML predictive models.\n",
        "\n",
        "3. **Model building and assessment** - The analyst builds the model on the training set and then uses the same model to create predictions on the test set. The analyst can then compare these predictions to the actual output in the test set.\n",
        "\n",
        "These series of steps is referred to as the **ML workflow**. Below we see step 1 and step 2 applied to a real world dataset where the task is to predict insurance claim of an individual given their characteristics."
      ],
      "metadata": {
        "id": "-ABXN4mym_KD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# step 1\n",
        "import pandas as pd\n",
        "insurance_data = (\n",
        "    pd.read_csv('https://raw.githubusercontent.com/pallavrouth/MarketingAnalytics/main/datasets/insurance.csv')\n",
        "      .drop(columns = ['index'])\n",
        "      .dropna(subset = ['age','region'])\n",
        ")\n",
        "\n",
        "insurance_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNvhCV3Zr6xW",
        "outputId": "bd969d32-70b3-496f-b3f8-56f9a212409f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1332, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "insurance_data.head()"
      ],
      "metadata": {
        "id": "57QGqtSAu8sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step 2\n",
        "\n",
        "# part 1: define your input and output\n",
        "target = insurance_data.loc[:,'claim']\n",
        "features = insurance_data.loc[:,['age','gender','bmi','bloodpressure','diabetic','children','smoker','region']]"
      ],
      "metadata": {
        "id": "02xB0aFIwj5O"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target"
      ],
      "metadata": {
        "id": "jUsljMksxF8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features"
      ],
      "metadata": {
        "id": "7j7TA-Q3xHaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# part 2: Optional. Sometimes you may need to process them\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "scaler = StandardScaler()\n",
        "encoder = OneHotEncoder(sparse_output = False)\n",
        "\n",
        "num_feats = ['age', 'bmi', 'bloodpressure']\n",
        "cat_feats = ['gender','diabetic','children', 'smoker', 'region']\n",
        "\n",
        "final_pipe = ColumnTransformer([\n",
        "   ('num', scaler, num_feats),\n",
        "   ('cat', encoder, cat_feats)\n",
        "])"
      ],
      "metadata": {
        "id": "xNheyQz-uy6b"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_processed = final_pipe.fit_transform(features)\n",
        "features_processed.shape"
      ],
      "metadata": {
        "id": "6HZl2etXvdAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# part 3: Split the data randomly into training and testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "features_train, features_test, target_train, target_test = train_test_split(features_processed, target, test_size = 0.3, random_state = 42)"
      ],
      "metadata": {
        "id": "-Q37B9Agunwa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(features_train.shape)\n",
        "print(features_test.shape)"
      ],
      "metadata": {
        "id": "eMZSr37LuYW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Supervised ML\n",
        "\n",
        "There are two types of models - regression versus classification. In regression problems, the goal is to predict a continuous or numerical outcome. This outcome can take any real-number value within a certain range, making it a quantitative prediction. In classification problems, the goal is to assign input data points to predefined categories or classes. The outcome is a categorical variable, and predictions are made by assigning each data point to one of these classes."
      ],
      "metadata": {
        "id": "3PpqEJKnsbWA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regression Models\n",
        "\n",
        "1. **Customer Lifetime Value (CLV) Prediction:** Predicting the future value of a customer over their entire relationship with a company. This can help businesses identify high-value customers, optimize marketing spend, and tailor their strategies to retain and acquire such customers.\n",
        "2. **Sales Forecasting:** Predicting future sales or revenue based on historical sales data, marketing campaigns, seasonality, and other relevant factors. Accurate sales forecasts help in inventory management, resource allocation, and budget planning.\n",
        "3. **Market Response Modeling:** Modeling the impact of marketing campaigns (e.g., advertising, promotions, email campaigns) on sales or customer acquisition. Understanding which marketing activities are most effective can help allocate resources more efficiently.\n",
        "4. **Consumer Response Modeling:** Modeling the impact of marketing campaigns (e.g., advertising, promotions, email campaigns) on sales or customer acquisition. Understanding which marketing activities are most effective can help allocate resources more efficiently.\n"
      ],
      "metadata": {
        "id": "XdUnYYYFHp9Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Linear Regression"
      ],
      "metadata": {
        "id": "BlD6_RhcHygq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# step 3: picking a suitable model - linear regression in this case\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# step 4: building and evaluating the model\n",
        "# part 1: build the model on training data\n",
        "model = LinearRegression()\n",
        "model.fit(features_train, target_train) # this is training data\n",
        "\n",
        "# use the test data to make predictions\n",
        "predicted_claim = model.predict(features_test) # this is test data"
      ],
      "metadata": {
        "id": "Mzi3RNOuzfVd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation and Error\n",
        "\n",
        "Recall I said that model assessment involves comparing model predictions to actual values of dependent variables in the test data. How do we compare exactly? Depending on the nature of dependent variable (categorical versus numerical), this involves using certain measures that capture **how far the predictions deviate from reality**. That is, we need to quantify the extent to which the predicted value of the dependent variable for a given observation is close to the true value of the dependent variable for that observation. These deviations are called **errors** in statistical terminology.\n",
        "\n",
        "Going back to our example of predicting customer's decision to stay, let's say the ML model created by the analyst predicts this decision for every single customer in the testing data set. The analyst is able to compare whether the predicted decision is equal to the actual decision. An error is generated is the ML model incorrectly classifies the decision (also called missclassification). The analyst can judge the performance of the models by pooling together all the the missclassifications or the errors.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fQWHdecivH8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# comparing actual versus predicted values of insurance claim\n",
        "(\n",
        "    pd.DataFrame({\n",
        "        'actual_claim' : list(target_test),\n",
        "        'predicted_claim' : list(predicted_claim)\n",
        "    })\n",
        ").head(n = 10)"
      ],
      "metadata": {
        "id": "NhWdjM3505tw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In regression settings, assessment is by calculating the **mean square error**. It is computed by subtracting the actual output from the predicted output. The difference is an error. The error tells us how close the actual output is from the predicted. We want to choose the method that gives the MSE, as opposed to the lowest training MSE."
      ],
      "metadata": {
        "id": "vMhqGDQdR_c4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# step 4: part 2: using a measure to evaluate the model\n",
        "mse = mean_squared_error(target_test, predicted_claim)\n",
        "print(\"The MSE from a Linear Regression model is \",mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lrmq3Sr801xS",
        "outputId": "24866408-aed1-4d65-e6b2-c81a69ff6e3d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The MSE from a Linear Regression model is  39571168.892895035\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we don't know if the overall error rate from the linear regression model (captured by the MSE value above) is any good. That is, we do not have anything to compare the error rate to.\n",
        "\n",
        "This brings us to an important aspect of being an analyst. A good analyst will build several different types of predictive ML models for the same task. A good model will have smaller deviations or smaller overall error rate. A worse model will have larger deviations or larger overall error rate. All ML predictive model algorithms are designed to make the errors as low as possible - sometimes they succeed, sometimes they don't. The analyst will then pick the one which has the lowest possible overall error.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "78PVC8FgSbMY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bias variance tradeoff\n",
        "\n",
        "When selecting an appropriate model from different alternative, the analyst often has to think about a bias variance trade off.\n",
        "\n",
        "- **Bias:** Bias refers to the error introduced by approximating a real-world problem, which may be complex, by a simplified model. A high bias model is overly simplistic and does not capture the underlying patterns in the data. This leads to **underfitting**, where the model is not flexible enough to represent the data accurately. In this case, the model consistently makes systematic errors, and it has a poor performance on both the training and test data.\n",
        "\n",
        "- **Variance:** Variance refers to the error introduced by a model that is too complex and captures noise in the training data. A high-variance model is highly flexible and may **overfit** the training data, capturing random fluctuations and noise rather than the underlying patterns. Such a model may perform very well on the training data but poorly on new, unseen data.\n",
        "\n",
        "The tradeoff can be summarized as follows:\n",
        "\n",
        "1. **High Bias, Low Variance:** Models with high bias and low variance are simple and tend to underfit the data. They have a systematic error that is consistent across different datasets.\n",
        "\n",
        "2. **Low Bias, High Variance:** Models with low bias and high variance are complex and tend to overfit the data. They are very flexible and can adapt to the noise in the training data, leading to poor generalization.\n",
        "\n",
        "Ideally we want low variance and low bias. In our example of predicting insurance we need to check whether the MSE from linear regression can be improved by other suitable alternatives. Below we look at some alternatives models that uses **regularization** to prevent overfitting. Then we check if the MSE from these models are better (lower)."
      ],
      "metadata": {
        "id": "6jqroE3kmCUT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regression Models with Regularization"
      ],
      "metadata": {
        "id": "FFvruU2wglPO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Lasso Regression\n",
        "\n",
        "Lasso and Ridge regression are two popular techniques that uses regularization used to improve linear regression models by addressing issues related to overfitting.\n",
        "\n",
        "Intuitively, regularization is a technique in machine learning helps prevent the problem of overfitting and improve the generalization of a model. It does so by adding a penalty or constraint to the model's optimization process, which influences the model's parameter estimates. This constraint encourages the model to avoid extreme or high values for the parameters or coefficients (which sometimes results in overfitting). By using this technique an analyst can identify which input variables were 'regularized' and can be subsequently dropped from the model."
      ],
      "metadata": {
        "id": "3HSNLFCmIUkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "alpha = 0.5 # this is the regularization parameter\n",
        "model = Lasso(alpha = alpha)\n",
        "model.fit(features_train, target_train)"
      ],
      "metadata": {
        "id": "zrJsoiUz2-zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# how regularization works to reduce coefficients\n",
        "coefficients = model.coef_\n",
        "feature_names = list(features.columns)\n",
        "\n",
        "feature_coefficients = [(feature_names[i], abs(coefficients[i])) for i in range(len(feature_names))]\n",
        "feature_coefficients.sort(key = lambda x: x[1], reverse = True)\n",
        "for feature, importance in feature_coefficients:\n",
        "    print(f\"Feature: {feature}, Importance: {importance}\")\n",
        ""
      ],
      "metadata": {
        "id": "rhPr-iSR45BW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check MSE from Lasso model\n",
        "predicted_claim = model.predict(features_test)\n",
        "mse = mean_squared_error(target_test, predicted_claim)\n",
        "print(\"The MSE from a Lasso model is \",mse)"
      ],
      "metadata": {
        "id": "G5eQhbZAoUQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ridge Regression\n",
        "\n",
        "Ridge regression is similar to Lasso regression in the sense that both types of regression are both regularization techniques used in linear regression to address common issues like overfitting and multicollinearity. The key difference is how they penalize less important input variables - while lasso uses \"L1\" regularization parameter, ridge uses \"L2\"."
      ],
      "metadata": {
        "id": "q3IwQjOLIWVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "alpha = 0.5\n",
        "model = Ridge(alpha = alpha)\n",
        "model.fit(features_train, target_train)"
      ],
      "metadata": {
        "id": "kvE9J88I6QlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_claim = model.predict(features_test)\n",
        "mse = mean_squared_error(target_test, predicted_claim)\n",
        "print(\"The MSE from a Ridge model is \",mse)"
      ],
      "metadata": {
        "id": "sLcjye957n9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing the MSE from linear regression, ridge and lasso, we can see that it is least for Lasso. This means that the error rate from is lowest when Lasso makes predictions. Therefore, it makes sense to pick this model for this particular prediction task."
      ],
      "metadata": {
        "id": "dB47A7JKqUpu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Within Model Evalution\n",
        "\n",
        "So far, we have used MSE to help the analyst make an important assessment - how well a model is performing compared to other models that are different. Therefore, this is a type of **between** model assessment.\n",
        "\n",
        "There is another type of assessment an analyst needs to consider - the **within** model assessment. This type of assessment involves picking the **best version of the same model**. This is often referred to as **model fine tuning**. In predictive ML, one can create different versions of a particular model by using the following tactics -\n",
        "\n",
        "1. **By adjusting it's hyper paramters** -  Hyperparameters are model parameters that are not learned from the data but are set prior to training by the analyst. They play a critical role in controlling how the model learns about the trends in the data and therefore, influences how well the predictions are. Therefore, selecting the appropriate value of hyper parameter is an important decision.\n",
        "\n",
        "  The best way to think about hyper parameters is to imagine you're driving a car, and the car's performance depends on several settings that you can adjust. These settings include the steering wheel's sensitivity, the pedal's responsiveness, the suspension's stiffness, and the engine's power. You can think of these settings as hyperparameters for your car.\n",
        "\n",
        "  Not all models have hyper parameter. In the example above, the linear regression does not have any hyper parameter. Lasso and ridge has a hyper parameter - the regularization parameter `alpha` the. By changing the value of `alpha` an analyst can create different versions of lasso and ridge model.\n",
        "\n",
        "2. **By adjusting the input variables** - The set of input or independent variables a model uses also influences how well it can learn to make predictions. A model with relevant input variables could have better predictions. Different models have different methods for guiding analysts to pick the best set of variables.\n",
        "\n",
        "  In our example above, both lasso and ridge models allows analyst to pick the best set of variables by automatically reducing the impact of variables that not important. In other models, this is a more manual process (see variable importance in random forest for example).\n",
        "\n",
        "Recall, when we were making **between model assessment** we were using a measure called MSE calculated with the help of the test data. This begs the question - should we carry out the **within model assessment** also on the test data ? The answer is no. For within model assessment **we rely on the training data**. Specifically, we re-split the training data into a smaller training data and a **validation data**.\n",
        "\n",
        "The validation data serves the same purpose as the test data. That is, an analyst builds models with different hyper parameters (or different inputs or both) on the smaller training data and evaluates them on the validation data. We can use the same type of measure - that is, MSE - to determine which hyper parameter is best.\n",
        "\n",
        "Instead of performing a simple split of the larger training data into into a smaller training data and a **validation data**, we will use something called the K-Fold Cross Validation.\n"
      ],
      "metadata": {
        "id": "Qwtbb2wSpCdx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K fold Cross Validation\n",
        "\n",
        "It involves dividing the dataset into K subsets (folds), training and testing the model K times, each time using a different fold as the test set and the remaining folds as the training set. Here's a step-by-step description of how K-fold CV is done:\n",
        "\n",
        "1. **Data Splitting:** Start with a dataset containing your features (input data) and target variable (output data). The first step is to divide this dataset into K roughly equal-sized subsets, or \"folds.\" The choice of K is determined by you; common values are 5 or 10.\n",
        "\n",
        "2. **Training and Testing:** Perform K iterations, where each iteration represents one \"fold.\" In each iteration, one of the K folds is used as the test set, while the other K-1 folds are used as the training set.\n",
        "\n",
        "3. **Model Training:** Train your machine learning model on the training set for the current iteration. This includes selecting the algorithm, specifying hyperparameters, and fitting the model to the training data.\n",
        "\n",
        "4. **Model Testing:** Use the trained model to make predictions on the test set for the current iteration. These predictions are used to evaluate the model's performance on unseen data.\n",
        "\n",
        "5. **Performance Metric:** Calculate a performance metric (e.g., accuracy, precision, recall, F1 score, or any relevant metric for your problem) based on the model's predictions and the true values in the test set. This metric assesses how well the model is doing in the current iteration.\n",
        "\n",
        "K fold Cross Validation can be better than a simple train test split. In K-fold CV each fold serves as the test set once. By averaging the results from multiple iterations, K-fold CV provides a more stable and less variable estimate of a model's performance compared to a single train-test split. Additionally, K fold CV can be useful when the dataset is small and there isn't enough samples to split into training and testing.\n",
        "\n",
        "Let's use K fold Cross Validation on Lasso to determine the best value of `alpha`."
      ],
      "metadata": {
        "id": "1gAwVfhhwyDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "\n",
        "alpha_values = [0.1, 0.5, 1, 10]\n",
        "\n",
        "# I will do a 3 fold CV\n",
        "kf = KFold(n_splits = 3, shuffle = True, random_state = 42)\n",
        "\n",
        "alpha_mse = []\n",
        "for alpha in alpha_values:\n",
        "  model = model = Lasso(alpha = alpha)\n",
        "  # perform the 3 fold CV on this model for this value of alpha\n",
        "  mse_scores = cross_val_score(model, features_train, target_train, cv = kf, scoring = 'neg_mean_squared_error') # notice I have used the training data here\n",
        "  mean_mse = - np.mean(mse_scores)\n",
        "  alpha_mse.append(f\"The MSE for alpha = {alpha} is {mean_mse}\")\n",
        "\n",
        "print(alpha_mse)"
      ],
      "metadata": {
        "id": "EmtVNGYBxGxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the MSE values above, we can see that the alpha value 10 of is most appropriate."
      ],
      "metadata": {
        "id": "_Z4ir7LByhkK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Revised ML workflow\n",
        "\n",
        "Lets revise the steps of our ML workflow taking into account within model assessment. The first three steps are the same.\n",
        "\n",
        "1. **Feature engineering**\n",
        "\n",
        "2. **Split the data**\n",
        "\n",
        "3. **Picking a suitable model**\n",
        "\n",
        "4. **Model fine tuning** - Use cross validation techniques on training data to tune the model. That is pick the best hyper parameter or the best set of input variables or both.\n",
        "\n",
        "3. **Model building and assessment** - The analyst builds the **tuned model** on the training set and then uses the same model to create predictions on the test set. The analyst can then compare these predictions to the actual output in the test set.\n",
        "\n",
        "In this example, we will use the best `alpha` value to re build the lasso model and check the performance on the test data."
      ],
      "metadata": {
        "id": "_dh_juET0eFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 10 # this is the regularization parameter that's best\n",
        "model = Lasso(alpha = alpha)\n",
        "model.fit(features_train, target_train)\n",
        "\n",
        "predicted_claim = model.predict(features_test)\n",
        "mse = mean_squared_error(target_test, predicted_claim)\n",
        "print(\"The MSE from a fine tuned Lasso model is \",mse)"
      ],
      "metadata": {
        "id": "koMYP4302bxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other regression models: KNN Regression\n",
        "\n",
        "K-Nearest Neighbors (KNN) regression is an algorithm that uses the concept of proximity to make predictions.\n",
        "\n",
        "1. **Data Points as Neighbors:** In KNN regression, your dataset consists of data points, each with multiple features (independent variables) and a target value (the value you want to predict). The \"neighbors\" in KNN are other data points in your dataset.\n",
        "\n",
        "2. **K and the Neighborhood:** The \"K\" in KNN represents the number of nearest neighbors you'll consider when making a prediction. For each data point, KNN finds the K nearest neighbors based on a distance metric, often Euclidean distance. These neighbors are the data points with the most similar feature values to the data point you're trying to predict.\n",
        "\n",
        "3. **Proximity-Based Prediction:** To make a prediction for a new data point, KNN averages (for regression) or takes a majority vote (for classification) of the target values of its K nearest neighbors. In the case of regression, it calculates the mean (average) of the target values of these neighbors."
      ],
      "metadata": {
        "id": "SqUCKNfWrrJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "knn_model = KNeighborsRegressor(n_neighbors = 3)\n",
        "knn_model.fit(features_train, target_train)"
      ],
      "metadata": {
        "id": "NY3SpmlGrqgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification Models\n",
        "\n",
        "1. **Churn Prediction:** Identifying customers who are likely to churn (stop using a product or service) versus those who are likely to stay loyal. This allows companies to focus retention efforts on high-risk customers.\n",
        "\n",
        "2. **Lead Scoring:** Classifying leads or prospects into categories like \"hot leads,\" \"warm leads,\" or \"cold leads\" based on their likelihood to convert into paying customers. This helps sales and marketing teams prioritize their efforts.\n",
        "\n",
        "3. **Spam Detection:** Classifying incoming emails, comments, or social media posts as either spam or legitimate content. This ensures that spam content is filtered out, providing a better user experience.\n",
        "\n",
        "4. **Sentiment Analysis:** Determining the sentiment (positive, negative, or neutral) of customer reviews, social media mentions, or other text data. This information helps gauge public perception and brand sentiment."
      ],
      "metadata": {
        "id": "nxmCHpz0st68"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression"
      ],
      "metadata": {
        "id": "tv9MBlEAH05M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# step 1\n",
        "churn_data = (\n",
        "    pd.read_csv('https://raw.githubusercontent.com/pallavrouth/MarketingAnalytics/main/datasets/churn.csv')\n",
        "      .drop(columns = ['RowNumber'])\n",
        ")\n",
        "\n",
        "churn_data.head()"
      ],
      "metadata": {
        "id": "x3-hg0AD9jhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step 2\n",
        "# part 1: define your input and output\n",
        "target = churn_data.loc[:,'Exited']\n",
        "features = churn_data.loc[:,['CreditScore', 'Geography', 'Gender', 'Age',\n",
        "                             'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',\n",
        "                             'IsActiveMember', 'EstimatedSalary']]\n",
        "\n",
        "# part 2: Optional. Sometimes you may need to process them\n",
        "scaler = StandardScaler()\n",
        "encoder = OneHotEncoder(sparse_output = False)\n",
        "\n",
        "num_feats = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']\n",
        "cat_feats = ['Geography', 'Gender','HasCrCard','IsActiveMember']\n",
        "\n",
        "final_pipe = ColumnTransformer([\n",
        "   ('num', scaler, num_feats),\n",
        "   ('cat', encoder, cat_feats)\n",
        "])\n",
        "\n",
        "features_processed = final_pipe.fit_transform(features)\n",
        "\n",
        "# part 3: Split the data randomly into training and testing\n",
        "features_train, features_test, target_train, target_test = train_test_split(features_processed, target, test_size = 0.3, random_state = 42)"
      ],
      "metadata": {
        "id": "BKc3ktby_vaH"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step 3: picking a suitable model - logistic regression in this case\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# step 4: building and evaluating the model\n",
        "# part 1: build the model on training data\n",
        "model = LogisticRegression()\n",
        "model.fit(features_train, target_train)\n",
        "\n",
        "predicted_probability = model.predict_proba(features_test)"
      ],
      "metadata": {
        "id": "mWqL1xFN_l5t"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_probability"
      ],
      "metadata": {
        "id": "8XESlYiEBEgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_label = model.predict(features_test)"
      ],
      "metadata": {
        "id": "9FPpDFQPEK_D"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_label"
      ],
      "metadata": {
        "id": "NPK9OWK1ENsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(\n",
        "    pd.DataFrame({\n",
        "        'actual_clickad' : list(target_test),\n",
        "        'predicted_clickad' : list(predicted_label)\n",
        "    })\n",
        ").head(n = 20)"
      ],
      "metadata": {
        "id": "Jq7HDHkEEZs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation for Classification Models\n",
        "\n",
        "A confusion matrix is a table used in machine learning and statistics to evaluate the performance of a classification algorithm. It is a valuable tool for assessing how well a model's predictions align with the actual class labels in a dataset. In a binary classification problem, where there are two possible classes (e.g., \"positive\" and \"negative\"), a confusion matrix typically consists of four values:\n",
        "\n",
        "1. **True Positives (TP):** The number of instances that were correctly predicted as positive (correctly classified as the positive class).\n",
        "2. **True Negatives (TN):** The number of instances that were correctly predicted as negative (correctly classified as the negative class).\n",
        "3. **False Positives (FP):** The number of instances that were incorrectly predicted as positive when they were actually negative (a type I error).\n",
        "4. **False Negatives (FN):** The number of instances that were incorrectly predicted as negative when they were actually positive (a type II error).\n",
        "\n",
        "The confusion matrix helps you calculate the following measures:\n",
        "\n",
        "1. **Accuracy:** (TP + TN) / (TP + TN + FP + FN), which measures the overall correctness of the classification.\n",
        "2. **Precision:** TP / (TP + FP), which quantifies the ability of the model to avoid false positive errors.\n",
        "3. **Recall** (Sensitivity or True Positive Rate): TP / (TP + FN), which measures the ability of the model to correctly identify positive instances.\n",
        "4. **F1 Score**: A metric that combines precision and recall to balance both false positives and false negatives.\n",
        "\n",
        "In this course, we will just focus on accuracy."
      ],
      "metadata": {
        "id": "R_guX0iDdRJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# step 4: part 2: using a measure to evaluate the model\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "confusion_mat = confusion_matrix(target_test, predicted_label)\n",
        "pd.DataFrame(confusion_mat, columns=[\"Predicted 0\", \"Predicted 1\"], index=[\"Actual 0\", \"Actual 1\"])"
      ],
      "metadata": {
        "id": "2v8qci6ooIl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(target_test, predicted_label)\n",
        "print(\"Accuracy of Logistic regression model is:\", accuracy)"
      ],
      "metadata": {
        "id": "23CnV0PbEhjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Support Vector Machines"
      ],
      "metadata": {
        "id": "rcGFAH-cH4tw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVMs are a type of Maximal Margin Classifier that uses a **hyperplane** to distinguish between two (or more classes).\n",
        "\n",
        "In a two-dimensional space (2D), a hyperplane is essentially a straight line that separates two classes of data points. In a three-dimensional space (3D), it becomes a flat plane. In higher-dimensional spaces, it's a higher-dimensional flat surface. The critical point is that a hyperplane is a decision boundary that separates data points belonging to different classes.\n",
        "\n",
        "The key idea behind the Maximal Margin Classifier is to find the hyperplane that maximizes the margin between the classes. The margin is the distance between the hyperplane and the nearest data points from each class. The data points that are closest to the hyperplane are called **\"support vectors.\"** These are the critical data points that define the margin. The distance between the support vectors and the hyperplane should be maximized."
      ],
      "metadata": {
        "id": "P2uxo2Bd25cW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM has a regularization parameter that needs to be tuned."
      ],
      "metadata": {
        "id": "8sRdxjrM1wh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "C_values = [0.1, 1, 10]\n",
        "\n",
        "best_model = None\n",
        "best_mean_accuracy = 0\n",
        "\n",
        "kf = KFold(n_splits = 3, shuffle = True, random_state = 42)\n",
        "\n",
        "for C in C_values:\n",
        "      # function for SVM\n",
        "      model = SVC(C = C, kernel = 'rbf', probability = True)\n",
        "      accuracy_scores = cross_val_score(model, features_train, target_train, cv = kf, scoring = 'accuracy')\n",
        "      mean_accuracy = np.mean(accuracy_scores)\n",
        "\n",
        "      if mean_accuracy > best_mean_accuracy:\n",
        "          best_mean_accuracy = mean_accuracy\n",
        "          best_model = model"
      ],
      "metadata": {
        "id": "SqKjFDrw11QJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the best model's hyperparameters and mean accuracy\n",
        "print(\"Best Model:\")\n",
        "print(best_model.get_params())\n",
        "print(\"Best Mean Accuracy:\", best_mean_accuracy)"
      ],
      "metadata": {
        "id": "0yDKIaE652BL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use the best hyper parameter to build the best model\n",
        "model = SVC(C = 1, kernel = 'rbf', probability = True)\n",
        "model.fit(features_train, target_train)\n",
        "\n",
        "predicted_label = model.predict(features_test)\n",
        "confusion_mat = confusion_matrix(target_test, predicted_label)\n",
        "print(pd.DataFrame(confusion_mat, columns=[\"Predicted 0\", \"Predicted 1\"], index=[\"Actual 0\", \"Actual 1\"]))\n",
        "\n",
        "accuracy = accuracy_score(target_test, predicted_label)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "4wdRgulNodGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Tree as Classifier\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1h1qzcygH_mw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "target = churn_data.loc[:,'Exited']\n",
        "features = churn_data.loc[:,['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']]\n",
        "\n",
        "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size = 0.3, random_state = 42)"
      ],
      "metadata": {
        "id": "Oa_-QgHjOtZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building a DT"
      ],
      "metadata": {
        "id": "KPud8aaL3Ur9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = DecisionTreeClassifier(max_depth = 3)\n",
        "model.fit(features_train, target_train)"
      ],
      "metadata": {
        "id": "dOmYZ4fsEK9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing a DT"
      ],
      "metadata": {
        "id": "qUPKwek9P91g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import export_graphviz\n",
        "import graphviz\n",
        "\n",
        "feature_names = list(features.columns)\n",
        "\n",
        "dot_data = export_graphviz(\n",
        "    model,\n",
        "    out_file = None,\n",
        "    feature_names = feature_names,\n",
        "    class_names = [\"churn\",\"active\"],\n",
        "    filled=True,\n",
        "    rounded=True,\n",
        "    special_characters=True\n",
        ")\n",
        "\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph"
      ],
      "metadata": {
        "id": "qbTCbWrvDxF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model tuning and prediction\n"
      ],
      "metadata": {
        "id": "HeL25A2XuaUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "\n",
        "max_depth = [3,6,9,12]\n",
        "\n",
        "best_model = None\n",
        "best_mean_accuracy = 0\n",
        "\n",
        "kf = KFold(n_splits = 3, shuffle = True, random_state = 42)\n",
        "\n",
        "for depth in max_depth:\n",
        "      model = DecisionTreeClassifier(max_depth = depth)\n",
        "      accuracy_scores = cross_val_score(model, features_train, target_train, cv = kf, scoring = 'accuracy')\n",
        "      mean_accuracy = np.mean(accuracy_scores)\n",
        "      print(\"For depth =\", depth, \"mean accuracy is \", mean_accuracy)\n",
        "\n",
        "      if mean_accuracy > best_mean_accuracy:\n",
        "          best_mean_accuracy = mean_accuracy\n",
        "          best_model = model\n",
        "\n",
        "# Print the best model's hyperparameters and mean accuracy\n",
        "print(\"Best Model:\")\n",
        "print(best_model)\n",
        "print(\"Best Mean Accuracy:\", best_mean_accuracy)"
      ],
      "metadata": {
        "id": "Q9wXg1LqQyV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using the tuned model for prediction**"
      ],
      "metadata": {
        "id": "EWBrwC4xRBCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "model = DecisionTreeClassifier(max_depth = 5)\n",
        "model.fit(features_train, target_train)\n",
        "\n",
        "predicted_label = model.predict(features_test)\n",
        "confusion_mat = confusion_matrix(target_test, predicted_label)\n",
        "pprint(pd.DataFrame(confusion_mat, columns=[\"Predicted 0\", \"Predicted 1\"], index=[\"Actual 0\", \"Actual 1\"]))\n",
        "\n",
        "accuracy = accuracy_score(target_test, predicted_label)\n",
        "print(\"Accuracy of a DT is:\", accuracy)"
      ],
      "metadata": {
        "id": "BW222EvZTd0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Tree as Regressor\n"
      ],
      "metadata": {
        "id": "zKSdyG8bU1lb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target = insurance_data.loc[:,'claim']\n",
        "features = insurance_data.loc[:,['age','bmi','bloodpressure','children']]\n",
        "\n",
        "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size = 0.3, random_state = 42)\n",
        "\n",
        "model = DecisionTreeRegressor(max_depth = 3, max_features = 3)\n",
        "model.fit(features_train, target_train)"
      ],
      "metadata": {
        "id": "eP8Xgg9XbU7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import export_graphviz\n",
        "import graphviz\n",
        "\n",
        "feature_names = list(features.columns)\n",
        "\n",
        "dot_data = export_graphviz(\n",
        "    model,\n",
        "    out_file = None,\n",
        "    feature_names = feature_names,\n",
        "    class_names = [\"churn\",\"active\"],\n",
        "    filled=True,\n",
        "    rounded=True,\n",
        "    special_characters=True\n",
        ")\n",
        "\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph"
      ],
      "metadata": {
        "id": "IiOdslDjdLrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyper-parameter tuning**\n",
        "\n"
      ],
      "metadata": {
        "id": "1wJGqaKednTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target = insurance_data.loc[:,'claim']\n",
        "features = insurance_data.loc[:,['age','gender','bmi','bloodpressure','diabetic','children','smoker','region']]\n",
        "\n",
        "num_feats = ['age', 'bmi', 'bloodpressure']\n",
        "cat_feats = ['gender','diabetic','children', 'smoker', 'region']\n",
        "\n",
        "encoder = OneHotEncoder(sparse_output = False)\n",
        "\n",
        "final_pipe = ColumnTransformer([\n",
        "   ('num', 'passthrough', num_feats),\n",
        "   ('cat', encoder, cat_feats)\n",
        "])\n",
        "\n",
        "\n",
        "final_pipe.fit(features)\n",
        "features_processed = final_pipe.transform(features)\n",
        "feature_names = final_pipe.get_feature_names_out()\n",
        "feature_names"
      ],
      "metadata": {
        "id": "ho9N4D4uU7w7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiple Hyper parameter tuning: Grid Search\n",
        "\n",
        "When a model has multiple hyper parameters to tune, we use a grid search technique to find the best set of hyper parameters."
      ],
      "metadata": {
        "id": "yWtHqqI-RXIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "max_depth = [3,5,7,9,12]\n",
        "max_feature = [3,5,7,9]\n",
        "\n",
        "kf = KFold(n_splits = 3, shuffle = True, random_state = 42)\n",
        "\n",
        "mean_mse_dfs = []\n",
        "for depth in max_depth:\n",
        "  for feature in max_feature:\n",
        "      model = DecisionTreeRegressor(max_depth = depth, max_features = feature)\n",
        "      mse_scores = cross_val_score(model, features_train, target_train, cv = kf, scoring = 'neg_mean_squared_error')\n",
        "      mean_mse = - np.mean(mse_scores)\n",
        "      mse_df = pd.DataFrame({'depth':[depth], 'max_feature':[feature], 'mean_mse': [mean_mse]})\n",
        "      mean_mse_dfs.append(mse_df)\n",
        "\n",
        "\n",
        "plot_data = (\n",
        "    pd.concat(mean_mse_dfs)\n",
        "      .assign(depth = lambda d: pd.Categorical(d.depth),\n",
        "              max_feature = lambda d: pd.Categorical(d.max_feature))\n",
        ")"
      ],
      "metadata": {
        "id": "pnaoHa8dazmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from plotnine import (\n",
        "    ggplot,\n",
        "    aes,\n",
        "    geom_bar,\n",
        "    coord_flip,\n",
        "    geom_point,\n",
        "    geom_line,\n",
        "    geom_tile,\n",
        ")\n",
        "\n",
        "(\n",
        "    ggplot(plot_data, aes(x = 'depth', y = 'max_feature', fill = 'mean_mse')) +\n",
        "           geom_tile(color = \"white\")\n",
        ")"
      ],
      "metadata": {
        "id": "YHBotwC2TJN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DecisionTreeRegressor(max_depth = 5, max_features = 7)\n",
        "model.fit(features_train, target_train)\n",
        "\n",
        "predicted_claim = model.predict(features_test)\n",
        "\n",
        "mse = mean_squared_error(target_test, predicted_claim)\n",
        "mse"
      ],
      "metadata": {
        "id": "VMBmmLI4y5uI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest\n",
        "\n"
      ],
      "metadata": {
        "id": "9VWKTWAfIDwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
        "\n",
        "target = insurance_data.loc[:,'claim']\n",
        "features = insurance_data.loc[:,['age','gender','bmi','bloodpressure','diabetic','children','smoker','region']]\n",
        "\n",
        "num_feats = ['age', 'bmi', 'bloodpressure']\n",
        "cat_feats = ['children']\n",
        "dummy_feats = ['gender','diabetic','smoker', 'region']\n",
        "\n",
        "one_hot_encoder = OneHotEncoder(sparse_output = False)\n",
        "ordinal_encoder = OrdinalEncoder()\n",
        "\n",
        "final_pipe = ColumnTransformer([\n",
        "   ('num', 'passthrough', num_feats),\n",
        "   ('cat', ordinal_encoder, cat_feats),\n",
        "   ('dummy', one_hot_encoder, dummy_feats)\n",
        "\n",
        "])\n",
        "\n",
        "\n",
        "final_pipe.fit(features)\n",
        "features_processed = final_pipe.transform(features)\n",
        "feature_names = final_pipe.get_feature_names_out()\n",
        "feature_names_clean = [fname.replace('num__','').replace('cat__','').replace('dummy__','') for fname in feature_names]"
      ],
      "metadata": {
        "id": "afO5aFAT23uE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_train, features_test, target_train, target_test = train_test_split(features_processed, target, test_size = 0.3, random_state = 42)\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "model = RandomForestRegressor(n_estimators = 250, max_features = 5)\n",
        "model.fit(features_train, target_train)\n",
        "\n",
        "predicted_claim = model.predict(features_test)\n",
        "mse = mean_squared_error(target_test, predicted_claim)\n",
        "mse"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htt81x5E274T",
        "outputId": "a4472531-6242-49dc-ab25-06aebe7f6faa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25435708.062197015"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variable Importance"
      ],
      "metadata": {
        "id": "zcoQobrEgdBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_data = pd.DataFrame({'features': feature_names_clean,\n",
        "                         'importances': model.feature_importances_})\n",
        "\n",
        "(\n",
        "    ggplot(plot_data, aes(x = 'reorder(features, importances)', y = 'importances')) +\n",
        "      geom_bar(stat = 'identity') +\n",
        "      coord_flip()\n",
        ")"
      ],
      "metadata": {
        "id": "N-Q1kZ_T3JbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target = insurance_data.loc[:,'claim']\n",
        "features = insurance_data.loc[:,['age','gender','bmi','bloodpressure','diabetic','children','smoker','region']]\n",
        "\n",
        "num_feats = ['age', 'bmi', 'bloodpressure']\n",
        "cat_feats = ['children']\n",
        "dummy_feats = ['smoker', 'region']\n",
        "\n",
        "one_hot_encoder = OneHotEncoder(sparse_output = False)\n",
        "ordinal_encoder = OrdinalEncoder()\n",
        "\n",
        "final_pipe = ColumnTransformer([\n",
        "   ('num', 'passthrough', num_feats),\n",
        "   ('cat', ordinal_encoder, cat_feats),\n",
        "   ('dummy', one_hot_encoder, dummy_feats)\n",
        "\n",
        "])\n",
        "\n",
        "\n",
        "final_pipe.fit(features)\n",
        "features_processed = final_pipe.transform(features)\n",
        "feature_names = final_pipe.get_feature_names_out()\n",
        "feature_names_clean = [fname.replace('num__','').replace('cat__','').replace('dummy__','') for fname in feature_names]\n",
        "feature_names_clean"
      ],
      "metadata": {
        "id": "NGtr9vw3cWJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RandomForestRegressor(n_estimators = 250, max_features = 5)\n",
        "model.fit(features_train, target_train)\n",
        "\n",
        "predicted_claim = model.predict(features_test)\n",
        "mse = mean_squared_error(target_test, predicted_claim)\n",
        "mse"
      ],
      "metadata": {
        "id": "aqBQhxzDiKod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Partial Dependence Plot"
      ],
      "metadata": {
        "id": "7n3xsodDOtu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.inspection import partial_dependence\n",
        "\n",
        "model = RandomForestRegressor(n_estimators = 250, max_features = 5)\n",
        "model.fit(features_train, target_train)"
      ],
      "metadata": {
        "id": "kWEykek5ie7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdep = partial_dependence(model, features = [0], X = features_train)\n",
        "pdep_df = pd.DataFrame({\n",
        "                       'pdep': pdep['average'].tolist()[0],\n",
        "                       'age': pdep['values'][0].tolist()\n",
        "                        })\n",
        "\n",
        "pdep_df\n",
        "\n",
        "(\n",
        "    ggplot(pdep_df, aes(x = 'age', y = 'pdep')) +\n",
        "      geom_point() +\n",
        "      geom_line()\n",
        ")"
      ],
      "metadata": {
        "id": "CyECE98VOkI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdep = partial_dependence(model, features = [1], X = features_train)\n",
        "pdep_df = pd.DataFrame({\n",
        "                       'pdep': pdep['average'].tolist()[0],\n",
        "                       'bmi': pdep['values'][0].tolist()\n",
        "                        })\n",
        "\n",
        "(\n",
        "    ggplot(pdep_df, aes(x = 'bmi', y = 'pdep')) +\n",
        "      geom_point() +\n",
        "      geom_line()\n",
        ")"
      ],
      "metadata": {
        "id": "CidrktcpRAbd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}